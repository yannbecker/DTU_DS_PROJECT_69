{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9600c1bc",
   "metadata": {},
   "source": [
    "# Embedding-based Scientific Article Recommendation System\n",
    "\n",
    "In this section, we implement the *embedding-based* component of our article recommendation pipeline.\n",
    "The goal is to take a short text fragment as input and return the **N most semantically similar scientific articles** \n",
    "from our arXiv-based dataset.\n",
    "\n",
    "We will:\n",
    "1. Load and preprocess the text data.\n",
    "2. Generate dense embeddings for all articles using a pre-trained model.\n",
    "3. Encode the input query into the same semantic space.\n",
    "4. Retrieve the top-N most similar articles using **K-Nearest Neighbors (KNN)** or **Approximate Nearest Neighbors (ANN)**.\n",
    "5. Evaluate the system performance using the **Top-N Accuracy** metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa69af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolar\\Desktop\\DTU\\Computational Tools for Data Science 02807\\CompToolsDataSc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# NLP and Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "\n",
    "\n",
    "# Visualization and evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc152b7a",
   "metadata": {},
   "source": [
    "Function: load_and_preprocess_data()\n",
    "\n",
    "Description:\n",
    "    Load and preprocess arXiv metadata from a large JSONL file.\n",
    "    Reads the arXiv dataset line by line (to handle very large files efficiently),\n",
    "    extracts essential fields, cleans the text from HTML tags and symbols, and\n",
    "    constructs a DataFrame containing one row per article.\n",
    "\n",
    "    The function also adds a 'categories' column, which may contain multiple\n",
    "    space-separated category labels such as 'math.PR math.AG'.\n",
    "\n",
    "Inputs:\n",
    "    data_path (str): Path to the JSONL file (each line is a separate JSON object).\n",
    "    max_rows (int, optional): Maximum number of rows to read (useful for testing\n",
    "                                on a subset). If None, reads the entire file.\n",
    "\n",
    "Outputs:\n",
    "    df (pd.DataFrame): DataFrame containing the following columns:\n",
    "        - 'id'         : arXiv identifier\n",
    "        - 'title'      : article title\n",
    "        - 'authors'    : author list (string)\n",
    "        - 'abstract'   : original abstract text\n",
    "        - 'clean_text' : preprocessed abstract text\n",
    "        - 'categories' : string of one or more categories (e.g. 'math.PR math.AG')\n",
    "\n",
    "Notes:\n",
    "    - Lines that cannot be decoded as JSON are skipped automatically.\n",
    "    - If an abstract is missing or empty, the entry is ignored.\n",
    "    - Special characters and LaTeX commands are stripped from 'clean_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d277fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path: str, max_rows: int = None) -> pd.DataFrame:\n",
    "    records = []\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows and i >= max_rows:\n",
    "                break\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                if \"abstract\" not in obj or not obj[\"abstract\"]:\n",
    "                    continue\n",
    "                abstract = obj[\"abstract\"]\n",
    "                clean_text = (\n",
    "                    re.sub(r\"<.*?>\", \"\", abstract)\n",
    "                    .replace(\"\\n\", \" \")\n",
    "                    .replace(\"\\\\\", \"\")\n",
    "                )\n",
    "                clean_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", clean_text).lower().strip()\n",
    "                categories = obj.get(\"categories\", \"\").strip()\n",
    "                records.append({\n",
    "                    \"id\": obj.get(\"id\", \"\"),\n",
    "                    \"title\": obj.get(\"title\", \"\"),\n",
    "                    \"authors\": obj.get(\"authors\", \"\"),\n",
    "                    \"abstract\": abstract,\n",
    "                    \"clean_text\": clean_text,\n",
    "                    \"categories\": categories\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9633eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles loaded: 2872766\n",
      "Columns: ['id', 'title', 'authors', 'abstract', 'clean_text', 'categories']\n",
      "          id                                              title  \\\n",
      "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
      "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
      "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
      "\n",
      "       categories  \n",
      "0          hep-ph  \n",
      "1   math.CO cs.CG  \n",
      "2  physics.gen-ph  \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "df = load_and_preprocess_data('./dataset/arxiv-metadata-oai-snapshot.json')\n",
    "\n",
    "print(\"Number of articles loaded:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head(3)[[\"id\", \"title\", \"categories\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a114727",
   "metadata": {},
   "source": [
    "Function: sample_by_top_categories()\n",
    "\n",
    "Description:\n",
    "Select a balanced sample of articles by top categories.\n",
    "    - Computes the frequency of each (primary) category in `df`.\n",
    "    - Selects the top-k categories by frequency.\n",
    "    - For each selected category, samples up to `n` articles.\n",
    "    - If some selected categories have fewer than `n` articles, attempts to\n",
    "        fill the remaining quota by drawing from the next-most-frequent categories,\n",
    "        in order, until approximately N = n * k articles have been collected\n",
    "        or the dataset is exhausted.\n",
    "    - Preserves uniqueness by 'id' (no duplicate articles in the output).\n",
    "\n",
    "Inputs:\n",
    "    df (pd.DataFrame): Input DataFrame containing at least the category column\n",
    "                        and an 'id' column.\n",
    "    k (int): Number of top categories to consider.\n",
    "    n (int): Desired number of articles per category (target).\n",
    "    category_col (str): Name of the column holding category information\n",
    "                        (default: \"categories\"). The cell expects strings\n",
    "                        like \"cs.AI cs.LG\" or \"hep-ph\".\n",
    "    primary_split (str): Delimiter to split multi-label category strings;\n",
    "                            default is space (\" \"). The first token becomes\n",
    "                            the \"primary\" category.\n",
    "    random_state (int): Random seed used for sampling for reproducibility.\n",
    "\n",
    "Outputs:\n",
    "    sampled_df (pd.DataFrame): DataFrame containing the sampled articles.\n",
    "    info (dict): Summary information including:\n",
    "        - 'selected_categories' : list of the final categories used (in order)\n",
    "        - 'counts_by_category'  : dict mapping category -> number sampled\n",
    "        - 'target_total'        : requested total (n * k)\n",
    "        - 'actual_total'        : actual number of returned articles\n",
    "\n",
    "Notes:\n",
    "    - The function will raise a ValueError if the category_col or 'id' column\n",
    "        are missing from `df`.\n",
    "    - If df contains categories as lists rather than strings, it will try to\n",
    "        handle them by taking the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b074417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_by_top_categories(df: pd.DataFrame, k: int, n: int,\n",
    "                             category_col: str = \"categories\",\n",
    "                             primary_split: str = \" \",\n",
    "                             random_state: int = 42) -> (pd.DataFrame, dict):\n",
    "    \n",
    "    if category_col not in df.columns:\n",
    "        raise ValueError(f\"DataFrame does not contain column '{category_col}'\")\n",
    "    if 'id' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain an 'id' column for uniqueness checks\")\n",
    "\n",
    "    # Create a primary category column\n",
    "    def _extract_primary(cat):\n",
    "        if pd.isna(cat):\n",
    "            return \"unknown\"\n",
    "        if isinstance(cat, list) and len(cat) > 0:\n",
    "            return str(cat[0]).strip()\n",
    "        s = str(cat).strip()\n",
    "        if s == \"\":\n",
    "            return \"unknown\"\n",
    "        return s.split(primary_split)[0] if primary_split in s else s\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"primary_category\"] = df[category_col].apply(_extract_primary)\n",
    "\n",
    "    # Compute counts and order categories\n",
    "    counts = df[\"primary_category\"].value_counts()\n",
    "    all_categories = counts.index.tolist()\n",
    "    if len(all_categories) == 0:\n",
    "        return pd.DataFrame(columns=df.columns), {\n",
    "            \"selected_categories\": [],\n",
    "            \"counts_by_category\": {},\n",
    "            \"target_total\": n * k,\n",
    "            \"actual_total\": 0\n",
    "        }\n",
    "\n",
    "    # Determine how many top categories we can actually use\n",
    "    k_adj = min(k, len(all_categories))\n",
    "    top_k_categories = all_categories[:k_adj]\n",
    "\n",
    "    target_total = n * k\n",
    "    sampled_ids = set()\n",
    "    sampled_rows = []\n",
    "    counts_by_category = {}\n",
    "\n",
    "    # First pass: take up to n from each of the top_k categories\n",
    "    for cat in top_k_categories:\n",
    "        cat_df = df[df[\"primary_category\"] == cat]\n",
    "        take = min(n, len(cat_df))\n",
    "        if take > 0:\n",
    "            sampled_cat = cat_df.sample(n=take, random_state=random_state)\n",
    "            # ensure uniqueness by id\n",
    "            sampled_cat = sampled_cat[~sampled_cat['id'].isin(sampled_ids)]\n",
    "            # update sets\n",
    "            sampled_rows.append(sampled_cat)\n",
    "            sampled_ids.update(sampled_cat['id'].tolist())\n",
    "            counts_by_category[cat] = len(sampled_cat)\n",
    "        else:\n",
    "            counts_by_category[cat] = 0\n",
    "\n",
    "    # If we still need more to reach target_total, iterate over remaining categories\n",
    "    if len(sampled_ids) < target_total:\n",
    "        remaining_needed = target_total - len(sampled_ids)\n",
    "        # get remaining categories in order of frequency (after the initial top_k)\n",
    "        remaining_categories = [c for c in all_categories if c not in top_k_categories]\n",
    "        # iterate and pull up to `n` from each until we fill or run out\n",
    "        for cat in remaining_categories:\n",
    "            if remaining_needed <= 0:\n",
    "                break\n",
    "            cat_df = df[df[\"primary_category\"] == cat]\n",
    "            # exclude already sampled ids\n",
    "            cat_df = cat_df[~cat_df['id'].isin(sampled_ids)]\n",
    "            if len(cat_df) == 0:\n",
    "                continue\n",
    "            take = min(n, len(cat_df), remaining_needed)\n",
    "            sampled_cat = cat_df.sample(n=take, random_state=random_state)\n",
    "            sampled_rows.append(sampled_cat)\n",
    "            sampled_ids.update(sampled_cat['id'].tolist())\n",
    "            counts_by_category[cat] = counts_by_category.get(cat, 0) + len(sampled_cat)\n",
    "            remaining_needed = target_total - len(sampled_ids)\n",
    "\n",
    "    # Concatenate sampled parts and return\n",
    "    if sampled_rows:\n",
    "        result_df = pd.concat(sampled_rows).drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    info = {\n",
    "        \"selected_categories\": list(counts.index[:k_adj]),\n",
    "        \"counts_by_category\": counts_by_category,\n",
    "        \"target_total\": target_total,\n",
    "        \"actual_total\": len(result_df)\n",
    "    }\n",
    "    return result_df, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e7127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target total (n*k): 100\n",
      "Actual retrieved: 100\n",
      "Selected categories: ['hep-ph', 'cs.CV', 'quant-ph', 'cs.LG', 'hep-th']\n",
      "Counts by category (sampled): {'hep-ph': 20, 'cs.CV': 20, 'quant-ph': 20, 'cs.LG': 20, 'hep-th': 20}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>categories</th>\n",
       "      <th>primary_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-ph/9502275</td>\n",
       "      <td>Nonleptonic Two-Body Decays of D Mesons in Bro...</td>\n",
       "      <td>Ian Hinchliffe and Thomas A. Kaeding</td>\n",
       "      <td>Decays of the D mesons to two pseudoscalars,...</td>\n",
       "      <td>decays of the d mesons to two pseudoscalars to...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1301.1123</td>\n",
       "      <td>Axions : Theory and Cosmological Role</td>\n",
       "      <td>Masahiro Kawasaki, Kazunori Nakayama</td>\n",
       "      <td>We review recent developments on axion cosmo...</td>\n",
       "      <td>we review recent developments on axion cosmolo...</td>\n",
       "      <td>hep-ph astro-ph.CO</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hep-ph/9607210</td>\n",
       "      <td>Froissart boundary for deep inelastic structur...</td>\n",
       "      <td>A.L.Ayala (IF UFRGS), M.B.Gay Ducati (IF UFRGS...</td>\n",
       "      <td>In this letter we derive the Froissart bound...</td>\n",
       "      <td>in this letter we derive the froissart boundar...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hep-ph/0604243</td>\n",
       "      <td>Higher $\\eta_c(nS)$ and $\\eta_b (nS)$ mesons</td>\n",
       "      <td>A.M.Badalian (Institute of Theoretical and Exp...</td>\n",
       "      <td>The hyperfine splittings in heavy quarkonia ...</td>\n",
       "      <td>the hyperfine splittings in heavy quarkonia ar...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1702.08417</td>\n",
       "      <td>Strong couplings and form factors of charmed m...</td>\n",
       "      <td>Alfonso Ballon-Bayona, Gastao Krein, Carlisson...</td>\n",
       "      <td>We extend the two-flavor hard-wall holograph...</td>\n",
       "      <td>we extend the twoflavor hardwall holographic m...</td>\n",
       "      <td>hep-ph hep-lat hep-th</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              title  \\\n",
       "0  hep-ph/9502275  Nonleptonic Two-Body Decays of D Mesons in Bro...   \n",
       "1       1301.1123              Axions : Theory and Cosmological Role   \n",
       "2  hep-ph/9607210  Froissart boundary for deep inelastic structur...   \n",
       "3  hep-ph/0604243       Higher $\\eta_c(nS)$ and $\\eta_b (nS)$ mesons   \n",
       "4      1702.08417  Strong couplings and form factors of charmed m...   \n",
       "\n",
       "                                             authors  \\\n",
       "0               Ian Hinchliffe and Thomas A. Kaeding   \n",
       "1               Masahiro Kawasaki, Kazunori Nakayama   \n",
       "2  A.L.Ayala (IF UFRGS), M.B.Gay Ducati (IF UFRGS...   \n",
       "3  A.M.Badalian (Institute of Theoretical and Exp...   \n",
       "4  Alfonso Ballon-Bayona, Gastao Krein, Carlisson...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    Decays of the D mesons to two pseudoscalars,...   \n",
       "1    We review recent developments on axion cosmo...   \n",
       "2    In this letter we derive the Froissart bound...   \n",
       "3    The hyperfine splittings in heavy quarkonia ...   \n",
       "4    We extend the two-flavor hard-wall holograph...   \n",
       "\n",
       "                                          clean_text             categories  \\\n",
       "0  decays of the d mesons to two pseudoscalars to...                 hep-ph   \n",
       "1  we review recent developments on axion cosmolo...     hep-ph astro-ph.CO   \n",
       "2  in this letter we derive the froissart boundar...                 hep-ph   \n",
       "3  the hyperfine splittings in heavy quarkonia ar...                 hep-ph   \n",
       "4  we extend the twoflavor hardwall holographic m...  hep-ph hep-lat hep-th   \n",
       "\n",
       "  primary_category  \n",
       "0           hep-ph  \n",
       "1           hep-ph  \n",
       "2           hep-ph  \n",
       "3           hep-ph  \n",
       "4           hep-ph  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled, summary = sample_by_top_categories(df, k=5, n=20, category_col=\"categories\", random_state=123)\n",
    "print(\"Target total (n*k):\", summary[\"target_total\"])\n",
    "print(\"Actual retrieved:\", summary[\"actual_total\"])\n",
    "print(\"Selected categories:\", summary[\"selected_categories\"])\n",
    "print(\"Counts by category (sampled):\", summary[\"counts_by_category\"])\n",
    "display(sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6e2be",
   "metadata": {},
   "source": [
    "Function: compute_and_save_embeddings()\n",
    "\n",
    "Description:\n",
    "    Computes sentence or document embeddings for a given collection of texts using\n",
    "    a pre-trained SentenceTransformer model, normalizes them for cosine similarity\n",
    "    computation, and saves both the embeddings and their corresponding document IDs\n",
    "    to disk. \n",
    "\n",
    "    This implementation is optimized for large datasets by using NumPy memory-mapped\n",
    "    arrays (memmaps), which prevent excessive RAM usage and allow writing embeddings\n",
    "    incrementally in batches.\n",
    "\n",
    "Inputs:\n",
    "    texts (List[str]): List of raw or preprocessed document texts to embed.\n",
    "    doc_ids (List[str]): List of unique document identifiers corresponding to the texts.\n",
    "    model_name (str): Name of the SentenceTransformer model to use\n",
    "                      (default: 'all-mpnet-base-v2').\n",
    "    batch_size (int): Number of documents processed per batch (default: 64).\n",
    "    out_emb_path (str): Path to save the resulting NumPy memmap file of embeddings (.npy).\n",
    "    out_ids_path (str): Path to save the document IDs (.npy).\n",
    "\n",
    "Outputs:\n",
    "    None. The function saves two files to disk:\n",
    "        - out_emb_path : A NumPy array of shape (N, d) containing normalized embeddings.\n",
    "        - out_ids_path : A NumPy array of shape (N,) containing corresponding document IDs.\n",
    "\n",
    "Notes:\n",
    "    - Each embedding vector is L2-normalized to ensure that cosine similarity \n",
    "      corresponds to inner product similarity.\n",
    "    - Using memmap storage allows the function to handle millions of documents efficiently.\n",
    "    - The SentenceTransformer model must be compatible with batch encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d91ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_save_embeddings(texts, doc_ids, model_name='all-mpnet-base-v2',\n",
    "                                batch_size=64, out_emb_path='embeddings.npy',\n",
    "                                out_ids_path='doc_ids.json', overwrite=True):\n",
    "    if os.path.exists(out_emb_path) and not overwrite:\n",
    "        raise FileExistsError(f\"{out_emb_path} already exists. Set overwrite=True to replace.\")\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    n = len(texts)\n",
    "    emb_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "    emb_memmap = np.lib.format.open_memmap(out_emb_path, mode='w+', dtype='float32', shape=(n, emb_dim))\n",
    "\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Embedding batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_emb = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "        # normalize rows to unit vectors (for cosine via inner product)\n",
    "        norms = np.linalg.norm(batch_emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        batch_emb = batch_emb / norms\n",
    "        emb_memmap[i:i+len(batch_emb)] = batch_emb.astype('float32')\n",
    "\n",
    "    # ensure data flushed to disk\n",
    "    del emb_memmap\n",
    "\n",
    "    # save doc ids as JSON\n",
    "    with open(out_ids_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(doc_ids), f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Saved embeddings -> {out_emb_path}\")\n",
    "    print(f\"Saved doc ids -> {out_ids_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7bc4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 13/13 [00:01<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings -> embeddings.npy\n",
      "Saved doc ids -> doc_ids.json\n",
      "Embeddings shape: (100, 384)\n",
      "Number of doc ids: 100\n",
      "Example embedding (first doc) first 10 dims: [-0.0930824  -0.08645523  0.02369268  0.05961437  0.01182459 -0.06102546\n",
      " -0.03752167  0.06795102 -0.05304568 -0.01412609]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = sampled[\"clean_text\"].tolist()\n",
    "doc_ids = sampled[\"id\"].tolist()\n",
    "\n",
    "compute_and_save_embeddings(\n",
    "    texts,\n",
    "    doc_ids,\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    batch_size=8,\n",
    "    out_emb_path='embeddings.npy',\n",
    "    out_ids_path='doc_ids.json',\n",
    "    overwrite=True\n",
    ")\n",
    "# Load and verify\n",
    "loaded_emb = np.load('embeddings.npy', mmap_mode='r')\n",
    "with open('doc_ids.json', 'r', encoding='utf-8') as f:\n",
    "    loaded_ids = json.load(f)\n",
    "\n",
    "print(\"Embeddings shape:\", loaded_emb.shape)\n",
    "print(\"Number of doc ids:\", len(loaded_ids))\n",
    "print(\"Example embedding (first doc) first 10 dims:\", loaded_emb[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927966bb",
   "metadata": {},
   "source": [
    "Function: build_faiss_index()\n",
    "\n",
    "Description:\n",
    "    Builds and saves a FAISS index for fast Approximate Nearest Neighbor (ANN) search\n",
    "    over precomputed document embeddings. This function loads a NumPy array of embeddings\n",
    "    and constructs an index structure optimized for efficient similarity queries.\n",
    "\n",
    "    Two types of FAISS indices are supported:\n",
    "        1. \"flat\"  – Exact search using a flat (brute-force) inner product index.\n",
    "                     Suitable for smaller datasets or evaluation baselines.\n",
    "        2. \"hnsw\"  – Hierarchical Navigable Small World (HNSW) graph-based index\n",
    "                     providing approximate nearest neighbor search with high recall\n",
    "                     and significantly faster query times for large datasets.\n",
    "\n",
    "    The index can later be loaded and used to efficiently retrieve the top-N most similar\n",
    "    items to a given query embedding.\n",
    "\n",
    "Inputs:\n",
    "    embeddings_path (str): Path to the NumPy file (.npy) containing the precomputed\n",
    "                           document embeddings of shape (N, d).\n",
    "    index_path (str): Path where the built FAISS index will be saved (.index file).\n",
    "    index_type (str): Type of FAISS index to build. Supported values:\n",
    "                      - 'flat' : Exact search (IndexFlatIP)\n",
    "                      - 'hnsw' : Approximate search (IndexHNSWFlat)\n",
    "    ef_construction (int): Construction parameter controlling recall vs. build time\n",
    "                           for HNSW. Higher values improve recall but increase\n",
    "                           build time and memory usage.\n",
    "    M (int): Number of bi-directional links created for each node in the HNSW graph.\n",
    "             Larger values increase recall but also memory footprint.\n",
    "\n",
    "Outputs:\n",
    "    faiss.Index: The constructed FAISS index object (also saved to disk at index_path).\n",
    "\n",
    "Notes:\n",
    "    - The embeddings should be normalized if cosine similarity is intended, since\n",
    "      FAISS IndexFlatIP and IndexHNSWFlat use inner product as the similarity metric.\n",
    "    - This function can handle large embedding matrices efficiently using memory mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8377153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings_path='embeddings.npy', index_path='faiss.index',\n",
    "                      index_type='hnsw', ef_construction=200, M=32):\n",
    "    emb = np.load(embeddings_path, mmap_mode='r')  # shape (N, d)\n",
    "    d = emb.shape[1]\n",
    "    if index_type == 'flat':\n",
    "        index = faiss.IndexFlatIP(d)  # inner product -> cosine if vectors normalized\n",
    "        index.add(emb)\n",
    "    elif index_type == 'hnsw':\n",
    "        index = faiss.IndexHNSWFlat(d, M)  # M controls connectivity\n",
    "        index.hnsw.efConstruction = ef_construction\n",
    "        index.add(emb)\n",
    "    else:\n",
    "        raise ValueError('index_type not supported')\n",
    "    faiss.write_index(index, index_path)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8187593",
   "metadata": {},
   "source": [
    "Function: retrieve_similar_articles()\n",
    "\n",
    "Description:\n",
    "    Given a query text (e.g., a fragment of scientific writing), this function embeds\n",
    "    the query in the same semantic space as the precomputed article embeddings and\n",
    "    retrieves the top-N most similar articles based on cosine similarity.\n",
    "\n",
    "    Two methods can be used:\n",
    "        (1) Exact K-Nearest Neighbors (KNN) search using sklearn’s NearestNeighbors\n",
    "        (2) Approximate Nearest Neighbors (ANN) search using Faiss\n",
    "    depending on the size of the dataset and computational constraints.\n",
    "\n",
    "Inputs:\n",
    "    query (str): The input text for which to find related articles.\n",
    "    model (SentenceTransformer): The embedding model used for encoding.\n",
    "    embeddings (numpy.ndarray): Precomputed embeddings of all documents.\n",
    "    articles (List[str]): Corresponding article IDs or titles.\n",
    "    top_n (int): Number of most similar articles to return.\n",
    "    use_ann (bool): If True, use approximate search (e.g., FAISS).\n",
    "\n",
    "Outputs:\n",
    "    pandas.DataFrame: A ranked table of the top-N retrieved articles with their\n",
    "                      similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e3e8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_articles(query, model, embeddings, articles, top_n=5, use_ann=False):\n",
    "    query_emb = model.encode([query], convert_to_numpy=True)\n",
    "    # normalize\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)\n",
    "    \n",
    "    if use_ann:\n",
    "        index = build_faiss_index()\n",
    "        distances, indices = index.search(query_emb.astype('float32'), top_n)\n",
    "    else:\n",
    "        # Exact search using sklearn\n",
    "        nbrs = NearestNeighbors(n_neighbors=top_n, metric=\"cosine\").fit(embeddings)\n",
    "        distances, indices = nbrs.kneighbors(query_emb)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"article_id\": [articles[i] for i in indices[0]],\n",
    "        \"similarity\": [1 - d for d in distances[0]]\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a4dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Recommended Articles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-ph/9502275</td>\n",
       "      <td>0.833745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1702.08417</td>\n",
       "      <td>0.304641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.12566</td>\n",
       "      <td>0.129331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1707.03249</td>\n",
       "      <td>-0.035064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hep-ph/0604243</td>\n",
       "      <td>-0.035543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id  similarity\n",
       "0  hep-ph/9502275    0.833745\n",
       "1      1702.08417    0.304641\n",
       "2      1904.12566    0.129331\n",
       "3      1707.03249   -0.035064\n",
       "4  hep-ph/0604243   -0.035543"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query taken from hep-ph/9502275\n",
    "query = \"Decays of the D mesons to two pseudoscalars to two vectors and to pseudoscalar plus vector are discussed in the context of broken flavor SU 3 A few assumptions are used to reduce the number of parameters\"\n",
    "top_n = 5\n",
    "\n",
    "embeddings = np.load('embeddings.npy', mmap_mode='r')\n",
    "results = retrieve_similar_articles(query, model, embeddings, doc_ids, top_n, use_ann=True)\n",
    "\n",
    "print(f\"Top-{top_n} Recommended Articles:\")\n",
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompToolsDataSc (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
